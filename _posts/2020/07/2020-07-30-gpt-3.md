---
toc:
  sidebar: true
giscus_comments: true
layout: post
title: "GPT-3是什么?"
date: "2020-07-30"
categories: 
  - "dl-ml-python"
  - "other"
  - "未分类"
---

很早前就想读一读火爆的GPT-3了, 今天刚好撸完了几十行很烂的代码下班早, 回来总结一下这篇占据各大AI学术头条的论文 **Language Models are Few-Shot Learners**[1], 中文翻译应该为**语言模型是小样本学习者**. 可以看到这篇论文光是作者就有31位, 论文不包括附录有40多页, 成果来自OpenAI.

![](https://zhengliangliang.files.wordpress.com/2020/07/screenshot-from-2020-07-30-20-26-44.png?w=1024)

啥是GPT-3? 首先GPT-3的全称叫**生成预训练转换器-3**(Generative Pretrained Transformer -3) . 简单来说他是一个巨大的语言模型, 只需要你在这个模型里面给定一个输入,它就能根据你的输入进行创作. 还没看懂吗? 那举个例子, 比方说你在输入写入:"今天天气很好", 它可能会输出"我和朋友们出去玩". 或者你输入"美国大选将在今年年末进行", 它可能会输出"特朗普推特表示将退出参选"(并不). 这就是GPT-3的机制, 它有个专有名词叫**自然语言生成**(Natural Language Generation). 可利用的领域很多,比方说**写新闻**, 比方说可以取代**淘宝客服**, 可以作为**聊天机器人**, **数位运算**...

**摘要**部分先是提到了一个常见的深度学习问题, 就是使用大量的语料数据和预训练可以得到较好的结果. 什么是预训练, 说白了就是说考试前先看看大量的小测题目,然后根据做这些题做到心里有数再去考试. 但是仍然需要大量的"做题". 然而对比人类来说, 学习语言, 我们只需要知道几个词(a few examples or from simple instructions), 就能进行变换表达各种意思. 文章提到作者们训练了一个高达**1750亿个参数的**模型(参数可以类比为人类的神经元), 这个模型在很多任务上都有好的结果, 比方说我刚刚提到的文字创作, 或者3位数的运算等. 文末探讨了这个模型对社会的影响.

俗话说得好, "孰能生巧","熟读唐诗三百首,不会作诗也会吟". 这些也正是如今人工智能界大部分人在做的东西,用**大量的训练**和**大量的数据集**来得到好的结果. 其实这也不难理解,高考刷题就是很好的一个印证. 强化学习中一个很著名的例子就是Alpha Go, 这个算法就是在网上看了所有能找到的棋盘(**大数据**),然后疯狂自己和自己下棋(**大量训练**). 这个GPT-3也不例外, 不妨先来猜一下他们都会用到什么数据, 如果我想让机器学写诗词, 那就看遍能找到的唐诗宋词, 我之前用循环神经网络(RNN)来训练过让计算机模仿我自己的写作,但是效果很差, 主要是因为我只放了几十篇自己以前写的小作文. 在这篇论文里面, 来自约翰霍普金斯和OpenAI的巨巨们用了一个叫普世抓取(Common Crawl)的数据集，基本就是抓取了现今互联网所有的内容(**4100亿个tokens**,这里的tokens可以理解为字数，也包括数字)．

大概了解了它是用什么数据之后，我们尝试来理解以下标题中**小样本学习者**是什么意思．小样本(few shots), 顾名思义就是样本少，那一般来说，少于100的都可以叫做小样本．与此对应的还有**零样本**以及**单样本**．下图是论文中给的例子．你只需要给定一个任务的描述(task description)，比方说将英文翻译为法语，然后换行后给一个输出标识(prompt),这就是零样本．单样本是啥意思呢？就是在刚刚的基础上加一行，sea otter => loutre de mer. 这样就有了一个可以参考的输出方式，依此类推，小样本就是给定多个例子．

![](https://zhengliangliang.files.wordpress.com/2020/08/screenshot-from-2020-07-31-22-36-13.png?w=441)

这里多嘴提一下这个模型开头提到了一个**微调**(Fine-Tuning)的概念，意思就是你预训练好了之后，这个模型可能比较大，但是做啥任务都一般般，你如果想让这个模型去写诗，你就让他再多看一些诗句的数据，它内部的参数就自动调整到适合这个任务去．但这个微调的弊端是每次你有新的任务，比方说，你又想让他去写新闻，那么你又要用一大堆新闻的资料数据让它继续微调到一个适合写新闻的模型．用机器学习的话来说，这个模型的**泛化程度**(generalization),或者说普世化很差．为了去掉这一个复杂的步骤，那应该怎么做？那就用多点数据来训练，让它做什么任务都很优秀．

除了刚刚上面提到的1750亿个参数的模型，文章里面为了对比还测试了其他不同量级参数的模型．比如下表中"宝宝类型"的GPT-3 small,只有1.25亿的参数．

![](https://zhengliangliang.files.wordpress.com/2020/08/screenshot-from-2020-07-31-22-57-01.png?w=1024)

关于模型部分，这里我想直接忽略，具体的模型和GPT-2一样，有兴趣的可以直接看文末分割线后的架构．

刚刚说到的数据集**一般爬取的数据集**(common crawl), 还有维基百科，海量书籍之类的．另外文章还有一小部分提到因为common crawl的数据集太大，4100亿个词，所以可能数据会被污染，意思就是在测试的时候可能用到的数据会来自训练集中，因为词太多难以过滤．所以这导致的一个现象是什么呢？就是说你期末考前做的题目已经是期末的题目了，那么你考试过程的分数就不太算你真正的"实力"，或者说不太能体现你的真实解题能力．这也许是本论文唯一一个明显的缺点了．那么后面我们就来罗列这个模型用于的测试任务很取得的一些结果．

我挑选了几个比较有意思的任务来介绍，第一个是**闭卷问题回答**(Closed Book Question Answering), 在搜索引擎中，我们的搜索结果其实是根据关键词匹配进行搜索或者相似度进行搜索的，这涉及到一门学科叫**信息检索**(Information Retrieval), 那关于开卷和闭卷在这里是什么意思呢? 文中的解释是**允许系统去搜索或者根据已经存在的答案进行回答的设定**叫做**开卷**，(this setting allows a system to search for and condition on text which potentially contains the answer it is denoted “open-book”.)，那么闭卷就是在回答的过程中是没有像开卷一样的辅助信息，需要进行一定的逻辑联系作答(我自己的理解)．

![](https://zhengliangliang.files.wordpress.com/2020/08/screenshot-from-2020-08-10-18-45-55.png?w=1024)

上图代表的是GPT-3在TriviaQA的数据集训练下的精确度，可以看到的是随着模型越来越大精确值也越来越高，在零样本的测试中就达到了64.3%的准确值．值得注意的是少样本(64个样本)和但样本的测试中在最1750亿的参数下是超过了RAG的，这个RAG模型是指的是可以去在线搜索的情况下得到的模型．这里反映了什么呢？就是说GPT-3在这个问答任务的表现上闭卷的答题能力甚至超过了其他现有开卷的最好模型．简单理解就是他是个**闭卷考一般性随机问题都能高过**的学霸．但是这个模型在科学自然问题表现不是太好，就是说涉及到事实，百科的问题就考得不是太好．

下一个任务是**翻译任务**，根据下图可以得出将其他语言转换成英语的准确度比较高，将英语转换成其他语言的准确率稍低一点，这也和英文的训练数据量比较大有关系．

![](https://zhengliangliang.files.wordpress.com/2020/08/screenshot-from-2020-08-10-22-39-04.png?w=1024)

下面一个比较有意思，是**常识推理**（Common Sense Reasoning), 其中的一个是选择题，里面内容是三年级到九年级的科学常识题. GPT-3 在零样本的情况下达到 51.4%的准确率，单样本是53.2%．比方说下面的问题，

有机体需要能量来做什么?

A. 发育生长(正确)

B.好好休息

C.吸收光源

D.摄取营养

![](https://zhengliangliang.files.wordpress.com/2020/08/screenshot-from-2020-08-10-22-50-24.png?w=1024)

**阅读理解**（Reading Comprehension）也是其中的一个考核，比如说给出下面这一篇文章，感觉还有点难理解呢．

Saint Jean de Brebeuf是是在1625前往新法兰西的一名法国耶稣会传教士。****除了1629年到1633年**待在法国之外，**他一直与怀恩多特人一起工作。他学习了他们的语言和文化，并广泛撰写了有关他们的文章，以帮助其他尼日尔主义者。 1649年，易洛魁族突袭占领了怀恩多特人村时，布雷博特和另一位传教士维雷尔被俘。 1649年3月16日，传教士与休伦俘虏一起在仪式上受到酷刑折磨并被杀。布鲁克比夫（Bróbeuf）在1925年被圣化，1930年在罗马天主教会中被封为圣徒的八名耶稣会士流亡者被册封。

**圣让·德布雷夫夫（Saint Jean deBrébeuf）在回到法国几年之前在新法兰西停留了多少年？**

![](https://zhengliangliang.files.wordpress.com/2020/08/screenshot-from-2020-08-10-22-51-09.png?w=1024)

其他的还有**自然语言推断(NLI)**, 主要意思是去理解两个句子之间的关联性．

其中还有一小部分是测试算法在数位运算的表现，但是因为计算能力比较差.我这里就略过了．

最重要的一个贡献，也是很多人关注的**是新闻和小说编写**,比方给你了小说的标题和副标题，GPT-3就可以进行创作．以下两幅图，第一个是人类很难判断是人写的还是机器写的，第二个是人类很容判断出区别．我在一个油管视频[2]看到有人把下面文章的一部分拿到谷歌上面去搜，发现是在某个书中出现的词汇，但是句子不是完全一样的，这就好像是人在阅读过一些书之后进行写作的过程就是知识重组和创意写作的一个过程．

**标题**：联合卫理公会同意历史性分裂

**副标题**：反对同性恋婚姻的人将组成自己的派名。

**文章**：经过两天的激烈辩论，联合卫理公会同意了历史性分裂-预期这一分裂将以历史的分裂结束。根据《华盛顿邮报》的说法，这是一种新的面貌，在“神学和社会上都是保守的”。出席5月教堂年度大会的大多数代表投票表决，加强了对LGBTQ神职人员的戒律的禁令，并制定了新的规则，以“纪律”主持同性婚礼的神职人员。但是那些反对这些措施的人有一个新计划：他们说到2020年，他们将形成一个单独的教派，将他们的教会称为基督教卫理公会。邮报指出，这个拥有1250万成员的教派在20世纪初是“美国最大的新教教派”，但近几十年来一直在萎缩。新的分裂将是教堂历史上的第二次分裂。第一次发生在1968年，当时约有10％的面额用于成立福音派弟兄会。邮报指出，提议的分裂“是在教堂的关键时刻出现的，该教堂已经失去会员多年了”。同性恋婚姻不是唯一使教会分裂的问题。2016年，美元的面额因跨性别的神职人员的划分而分裂，北太平洋区域会议投票禁止他们担任神职人员，南太平洋区域会议投票允许他们。

![](https://zhengliangliang.files.wordpress.com/2020/08/screenshot-from-2020-08-10-23-11-13.png?w=1024)

**下面的这篇文章很容易被本地人看出是假的新闻，英文Megyn Kelly的节目不是The tonight show, 这个节目的主持人是Jimmy Fallon．**

**标题：明星的礼服的承诺受到了梅根·凯利（Megyn Kelly）的嘲讽**

**副标题**：**华金·菲尼克斯（Joaquin Phoenix）承诺不会为每项颁奖活动而改变**

**文章:** 一年前，华金·菲尼克斯（Joaquin Phoenix）穿着无尾礼服，头戴纸袋的燕尾服出现在金球奖的红地毯上，上面写着：“我是一个变形者。我无法改变世界。我可以只改变自己。”承诺不会改变以适应好莱坞的模样：“我认为这是一件非常特别的事情，不要改变自己。我认为这是一件非常特别的事情，”这就是我的内心，我为此感到自豪我不会因为别人认为我应该的样子而感到羞愧。”“”现在是奥斯卡，而菲尼克斯再次出现了。但是这次，他的公关人员说无论如何他都会穿着燕尾服。

梅根·凯利（Megyn Kelly）没有很惊讶，她让他在《今夜秀》上也穿着燕尾服。她说：“你知道，我觉得你本可以穿燕尾服。” “但是你是说你是一个变形者。我不知道你是否可以改变晚礼服，但是你可以改变主意。你可以改变主意。你可以改变主意。”菲尼克斯说他做到了，但是没有坚持。 “我当时想，'Dkay，我要为这件衣服穿燕尾服。'然后我想，“我不想为这件事情穿燕尾服。”“凯利继续鼓励他再次改变主意，但菲尼克斯说为时已晚：“我致力于穿这件衣服。”

![](https://zhengliangliang.files.wordpress.com/2020/08/screenshot-from-2020-08-10-23-11-22.png?w=1024)

open AI 官网还有几个很fancy的视频，大家可以去看看，比如说表格生成，比如说在维基百科的某个词条下问问题就会自动跳转到相应的答案去...

OpenAI API. [https://openai.com/blog/openai-api/](https://openai.com/blog/openai-api/)

![](https://zhengliangliang.files.wordpress.com/2020/08/screenshot-from-2020-08-10-23-34-38.png?w=1024)

另外OpenAI API这个模型可以去申请GPT-3的使用权，只需要填一个表就可以了，但是要审核，我在推特上看到有些人通过了审核拿这个模型来作诗搞音乐啥的，我上一周就申请了，但是还没有回复. sigh...

参考文献:

[1] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Agarwal, S. (2020). Language models are few-shot learners. _arXiv preprint arXiv:2005.14165_.  
[2] Yannic K, [Yannic Kilcher]. (2020,May 29). GPT-3: Language Models are Few-Shot Learners (Paper Explained) [Video file]. Retrieved from [](https://www.youtube.com/watch?v=SY5PvZrJhLE)[https://www.youtube.com/watch?v=SY5PvZrJhLE](https://www.youtube.com/watch?v=SY5PvZrJhLE)
